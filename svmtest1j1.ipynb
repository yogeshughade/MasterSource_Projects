{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve, f1_score \n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#if the package are not installed run following command\n",
    "#pip install pandas numpy matplotlib seaborn scikit-learn\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/PC-LENOVO/OneDrive/Desktop/anomaly detetction/02-14-2018/data.csv\") #CSE-CIC-IDS 2018 (02-20-2018)\n",
    "df\n",
    "print(df.head())\n",
    "cols = df.columns\n",
    "cols = cols.map(lambda x: x.replace(' ', '_') )\n",
    "df.columns = cols\n",
    "print(df.head())\n",
    "query = df.query('Dst_Port == 80 or Dst_Port == 443')\n",
    "df=query\n",
    "print('Number of rows:', df.shape[0])\n",
    "# check for null\n",
    "df.isnull().any()\n",
    "# counting infinity in a particular column name\n",
    "inf=df.isin([np.inf, -np.inf])\n",
    "inf\n",
    "#replace infinit number\n",
    "df=df.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "##df = df[np.isfinite(df).all(1)]\n",
    "#drop null\n",
    "df.dropna(how = 'all')\n",
    "print('Data type of each column of Dataframe :')\n",
    "df.info(verbose=True)\n",
    "df = df.drop(columns=['Timestamp', 'Flow_ID', 'Src_IP', 'Dst_IP'])\n",
    "df\n",
    "print(df['Label'].value_counts())\n",
    "df.Label[df.Label=='Benign'] = 0\n",
    "df.Label[df.Label =='DDoS attacks-LOIC-HTTP'] = 1\n",
    "print(df['Label'].value_counts())\n",
    "# Count the number of rows in each class\n",
    "ddos_count = df[df['Label'] == 1].shape[0]\n",
    "benign_count = df[df['Label'] == 0].shape[0]\n",
    "\n",
    "# Randomly sample the benign rows to reduce their count\n",
    "df_benign = df[df['Label'] == 0].sample(n=ddos_count, random_state=42)\n",
    "\n",
    "# Combine the DDoS and sampled benign rows into a new dataframe\n",
    "df_reduced = pd.concat([df[df['Label'] == 1], df_benign])\n",
    "\n",
    "# Shuffle the rows in the new dataframe\n",
    "df_reduced = df_reduced.sample(frac=1, random_state=42)\n",
    "\n",
    "df = df_reduced\n",
    "\n",
    "print(df['Label'].value_counts())\n",
    "bening_df = df[df['Label']==0]\n",
    "malignant_df = df[df['Label']==1]\n",
    "axes = bening_df.plot(kind='scatter', x='Flow_Duration', y = 'Tot_Fwd_Pkts', color='blue', label='Benign')\n",
    "malignant_df.plot(kind='scatter', x='Flow_Duration', y = 'Tot_Fwd_Pkts', color='red', label='maligmant', ax=axes)\n",
    "# Shuffle the rows\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Select a subset of the data for training\n",
    "num_data = 2000 # can change between 500 and 1152382\n",
    "train_df = df.iloc[:num_data].copy() # Use .iloc to avoid a SettingWithCopyWarning\n",
    "train_df = train_df.astype(\"float64\")\n",
    "\n",
    "print('Data type of each column of Dataframe :')\n",
    "train_df.info(verbose=True)\n",
    "df.columns\n",
    "# Remove Label column from train_df and store it in target variable as nparray\n",
    "target = np.asanyarray(train_df.pop('Label'))\n",
    "\n",
    "# create train_df nparray varibel\n",
    "raw = np.asanyarray(train_df)\n",
    "\n",
    "print(\"target array: \\n\", target,\"\\n\\n\",\"raw array: \\n\", raw)\n",
    "# replace infinite values with a large finite value\n",
    "raw[~np.isfinite(raw)] = np.finfo(raw.dtype).max\n",
    "\n",
    "# replace NaN values with zero\n",
    "raw = np.nan_to_num(raw)\n",
    "\n",
    "raw = np.asanyarray(train_df)\n",
    "print(\"is raw isinf: \\n\",np.isinf(raw),\"\\n\")\n",
    "print(\"is raw isfinite: \\n\",np.isfinite(raw),\"\\n\")\n",
    "print(\"is raw nan: \\n\", np.isnan(raw),\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.3 # number of training raws\n",
    "random_state = 42 # random seed\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(raw, target, test_size=test_size, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'kernel': ['rbf'],\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# create a GridSearchCV object\n",
    "svm_model = SVC()\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=50, n_jobs=-1, verbose=1)\n",
    "\n",
    "# fit the GridSearchCV object to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print the best hyperparameters\n",
    "print(\"Best kernel:\", grid_search.best_params_['kernel'])\n",
    "print(\"Best C:\", grid_search.best_params_['C'])\n",
    "print(\"Best gamma:\", grid_search.best_params_['gamma'],\"\\n\\n\")\n",
    "\n",
    "# Get the best SVM model from the grid search\n",
    "best_svm_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_svm_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance on the test set\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred))\n",
    "print('Recall:', recall_score(y_test, y_pred))\n",
    "print('F1 Score:', f1_score(y_test, y_pred))\n",
    "# Extract the mean cross-validation scores for each combination of hyperparameters\n",
    "scores = grid_search.cv_results_['mean_test_score'].reshape(len(param_grid['C']), len(param_grid['gamma']))\n",
    "\n",
    "# Set up the figure and axes objects\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Create a heatmap of the mean cross-validation scores\n",
    "im = ax.imshow(scores, cmap='YlGn', interpolation='nearest', vmin=0, vmax=1)\n",
    "\n",
    "# Set the axis labels and ticks\n",
    "ax.set_xlabel('Gamma Value', fontsize=14)\n",
    "ax.set_ylabel('C Value', fontsize=14)\n",
    "ax.set_xticks(np.arange(len(param_grid['gamma'])))\n",
    "ax.set_yticks(np.arange(len(param_grid['C'])))\n",
    "ax.set_xticklabels(param_grid['gamma'], fontsize=12, rotation=45)\n",
    "ax.set_yticklabels(param_grid['C'], fontsize=12)\n",
    "\n",
    "# Add annotations for the score values\n",
    "for i in range(len(param_grid['C'])):\n",
    "    for j in range(len(param_grid['gamma'])):\n",
    "        text = ax.text(j, i, '{:.2f}'.format(scores[i, j]), ha='center', va='center', color='k', fontsize=12)\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = ax.figure.colorbar(im, ax=ax)\n",
    "cbar.ax.set_ylabel('Mean CV Score', fontsize=14)\n",
    "# Set the title of the plot\n",
    "ax.set_title('Grid Search CV Scores for SVM', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt='g', \n",
    "            xticklabels=[0,1], yticklabels=[0,1])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Compute the accuracy score\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nAccuracy:\", acc_score)\n",
    "\n",
    "\n",
    "# Train the SVM model and record the training time\n",
    "start_time = time.time()\n",
    "best_svm_model.fit(X_train, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "# Predict on the test set and record the prediction time\n",
    "start_time = time.time()\n",
    "y_pred = best_svm_model.predict(X_test)\n",
    "pred_time = time.time() - start_time\n",
    "\n",
    "\n",
    "# Plot the accuracy over time and accuracy\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "fig.suptitle('Accuracy Analysis')\n",
    "\n",
    "# Plot accuracy over time\n",
    "ax1.plot([train_time + pred_time], [acc_score], 'bo', label='Accuracy')\n",
    "ax1.axhline(y=acc_score, color='gray', linestyle='--')\n",
    "ax1.axvline(x=train_time + pred_time, color='gray', linestyle='--')\n",
    "ax1.set_xlim([0, train_time + pred_time + 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_xlabel('Time (s)')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot accuracy\n",
    "ax2.bar(['SVM'], [acc_score], color='cornflowerblue')\n",
    "ax2.plot([-0.5, 0.5], [acc_score, acc_score], 'k--', label='Accuracy')\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()\n",
    "# Classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification report:\\n\", class_report)\n",
    "#plot bar graphs based on data\n",
    "\n",
    "#create dictiona\n",
    "class_report_dict = {\n",
    "    '0.0': {'precision': 0.98, 'recall': 0.93, 'f1-score': 0.96, 'support': 324},\n",
    "    '1.0': {'precision': 0.92, 'recall': 0.98, 'f1-score': 0.95, 'support': 276}\n",
    "}\n",
    "# Set the color palette\n",
    "palette = sns.color_palette('pastel')\n",
    "\n",
    "# Get the class names and metrics\n",
    "class_names = sorted(list(class_report_dict.keys()))\n",
    "metrics = ['precision', 'recall', 'f1-score']\n",
    "\n",
    "# Create a figure with subplots for each metric\n",
    "fig, axs = plt.subplots(1, len(metrics), figsize=(15, 5))\n",
    "\n",
    "# Plot bar graphs for each metric and class\n",
    "for i, metric in enumerate(metrics):\n",
    "    scores = [class_report_dict[class_name][metric] for class_name in class_names]\n",
    "    ax = sns.barplot(x=class_names, y=scores, ax=axs[i], palette=palette)\n",
    "    ax.set(title=metric.capitalize() + ' by class', ylabel=metric.capitalize(), ylim=(0, 1), facecolor='white')\n",
    "\n",
    "# Set the overall title\n",
    "fig.suptitle('Classification Report', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "# ROC curve\n",
    "y_prob = best_svm_model.decision_function(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "#print('fpr value:', fpr, '\\n')\n",
    "#print('tpr value:', tpr, '\\n')\n",
    "#print('thresholds value:', thresholds, '\\n')\n",
    "\n",
    "# Precision-Recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "pr_auc = roc_auc_score(y_test, y_prob)\n",
    "plt.figure()\n",
    "plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-Recall curve: AP={0:0.2f}'.format(pr_auc))\n",
    "plt.show()\n",
    "\n",
    "#print(\"Precision values:\", precision, '\\n')\n",
    "#print(\"Recall values:\", recall,'\\n')\n",
    "#print(\"AUC of Precision-Recall curve:\", pr_auc)\n",
    "\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1 score:\", f1)\n",
    "\n",
    "# Vary the C parameter and calculate F1-score for each value\n",
    "C_values = [0.1, 1, 10, 100]\n",
    "f1_scores = []\n",
    "for c in C_values:\n",
    "    svm_model = SVC(kernel='rbf', C=c, gamma='scale')\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    y_pred = svm_model.predict(X_test)\n",
    "    f1_scores.append(f1_score(y_test, y_pred))\n",
    "    print(\"C:\", c, \"F1 score:\", f1_scores[-1])\n",
    "\n",
    "# Plot the F1-scores against the C values\n",
    "plt.plot(C_values, f1_scores)\n",
    "plt.title('F1-score for SVM model')\n",
    "plt.xlabel('C values')\n",
    "plt.ylabel('F1-score')\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(C_values)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# transform the training and testing data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
